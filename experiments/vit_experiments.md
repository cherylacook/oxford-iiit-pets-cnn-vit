# ViTExperiments 

## Baseline of 5 transformer layers, 5 attention heads, patch size of 8, with positional embeddings: 56% test accuracy

**Only one parameter varied at a time from baseline.**
